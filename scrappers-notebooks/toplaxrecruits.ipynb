{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bilal/.local/lib/python3.8/site-packages/chromedriver_autoinstaller/106/chromedriver'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chromedriver_autoinstaller.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scrape_link=\"https://toplaxrecruits.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_name=\"../testing/toplaxrecruits\"\n",
    "if (not os.path.exists(output_folder_name)):\n",
    "    os.mkdir(output_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting https://toplaxrecruits.com/\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting {scrape_link}\")\n",
    "driver.get(scrape_link)\n",
    "sleep(2)\n",
    "menu_items=[\"menu-item-105016\",\"menu-item-105007\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting all the sub menus for girls and boys commitments\n",
      "2025 Boys\n",
      "https://toplaxrecruits.com/2025-boys\n",
      "2024 Boys\n",
      "https://toplaxrecruits.com/2024-boys\n",
      "2023 Boys\n",
      "https://toplaxrecruits.com/2023-boys-commits\n",
      "2025 Girls\n",
      "https://toplaxrecruits.com/2025-girls\n",
      "2024 Girls\n",
      "https://toplaxrecruits.com/2024-girls\n",
      "2023 Girls\n",
      "https://toplaxrecruits.com/2023-girls\n"
     ]
    }
   ],
   "source": [
    "categories={}\n",
    "\n",
    "print(\"getting all the sub menus for girls and boys commitments\")\n",
    "for menu in menu_items:\n",
    "    \n",
    "    comitments = driver.find_element(By.XPATH,f'.//li[@id=\"{menu}\"]')\n",
    "    # Create an ActionChains object\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    # Hover over the element\n",
    "    actions.move_to_element(comitments).perform()\n",
    "    sleep(1)\n",
    "    sub_list=comitments.find_elements(By.XPATH,\".//li\")\n",
    "    len(sub_list)\n",
    "    for sub_menu in sub_list:\n",
    "        print(sub_menu.text)\n",
    "        href_value = sub_menu.find_element(By.XPATH,\".//a\").get_attribute(\"href\")\n",
    "        print(href_value)\n",
    "\n",
    "        categories[sub_menu.text]=href_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping data for 2025 Boys\n",
      "\n",
      "show 100 records per page\n",
      "\n",
      "scrolling through all the pages....\n",
      "\n",
      "(473, 8) records found\n",
      "\n",
      "csv SAVED\n",
      "scrapping data for 2024 Boys\n",
      "\n",
      "show 100 records per page\n",
      "\n",
      "scrolling through all the pages....\n",
      "\n",
      "(1745, 7) records found\n",
      "\n",
      "csv SAVED\n",
      "scrapping data for 2023 Boys\n",
      "\n",
      "show 100 records per page\n",
      "\n",
      "scrolling through all the pages....\n",
      "\n",
      "(2822, 12) records found\n",
      "\n",
      "csv SAVED\n",
      "scrapping data for 2025 Girls\n",
      "\n",
      "show 100 records per page\n",
      "\n",
      "scrolling through all the pages....\n",
      "\n",
      "(297, 9) records found\n",
      "\n",
      "csv SAVED\n",
      "scrapping data for 2024 Girls\n",
      "\n",
      "show 100 records per page\n",
      "\n",
      "scrolling through all the pages....\n",
      "\n",
      "(1042, 7) records found\n",
      "\n",
      "csv SAVED\n",
      "scrapping data for 2023 Girls\n",
      "\n",
      "show 100 records per page\n",
      "\n",
      "scrolling through all the pages....\n",
      "\n",
      "(1003, 7) records found\n",
      "\n",
      "csv SAVED\n"
     ]
    }
   ],
   "source": [
    "for cat in categories.keys():\n",
    "# cat=\"2024 Boys\"\n",
    "    print(f\"scrapping data for {cat}\\n\")\n",
    "    driver.get(categories[cat])\n",
    "    sleep(2)\n",
    "    print(\"show 100 records per page\\n\")\n",
    "    select = Select(driver.find_element(By.XPATH,'.//select'))\n",
    "\n",
    "    select.select_by_value(\"100\")\n",
    "    sleep(2)\n",
    "    dfs_list=[]\n",
    "    print(\"scrolling through all the pages....\\n\")\n",
    "    while True:\n",
    "        sleep(3)\n",
    "\n",
    "        driver.find_element(By.XPATH,'.//table')\n",
    "\n",
    "        results=driver.find_elements(By.XPATH,'.//table')\n",
    "        table_html = results[-1].get_attribute(\"outerHTML\")\n",
    "\n",
    "        # Use Pandas to read HTML and convert it to a DataFrame\n",
    "        df = pd.read_html(table_html)[0]\n",
    "        df = df.dropna(axis = 0, how = 'all')\n",
    "        if df.shape[0] == 0:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            df.drop(['Unnamed: 8'], axis = 1, inplace = True) \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        dfs_list.append(df)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH,'.//a[@class=\"paginate_button next\"]').click()\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "\n",
    "    result_df=pd.concat(dfs_list, axis=0, ignore_index=True)\n",
    "    result_df\n",
    "    print(f\"{result_df.shape} records found\\n\")\n",
    "\n",
    "    result_df.to_csv(f\"{output_folder_name}/{cat.replace(' ','_').lower()}.csv\", index =False)\n",
    "\n",
    "    print(\"csv SAVED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "print(\"completed\")\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
