{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "opt = webdriver.ChromeOptions()\n",
    "opt.add_argument(\"--start-maximized\")\n",
    "opt.add_experimental_option(\"excludeSwitches\", [\"disable-popup-blocking\"])\n",
    "# opt.add_argument(\"--headless\")\n",
    "chromedriver_autoinstaller.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_CONFIG_FLAG = False\n",
    "SAVE_DATA_FLAG = True\n",
    "UPDATE_CONFIG_FLAG=True\n",
    "# value in config file status column to trigger scrapper\n",
    "STATUS_VALUE_TO_SCRAPE=['No','Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down_page(driver, speed=8,sleep_interval=1):\n",
    "    current_scroll_position, new_height= 0, 1\n",
    "    while current_scroll_position <= new_height:\n",
    "        current_scroll_position += speed\n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(current_scroll_position))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down_slowly(driver, speed=3000, sleep_interval=3,scrolling_steps=20):\n",
    "    y = speed\n",
    "    for timer in range(0,scrolling_steps):\n",
    "        driver.execute_script(\"window.scrollTo(0, \"+str(y)+\")\")\n",
    "        y += speed  \n",
    "        sleep(sleep_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dynamic_main_table(\n",
    "    web_driver,\n",
    "    url,\n",
    "    sleep_interval,\n",
    "    speed=3000,\n",
    "    scroll_sleep_interval=3,\n",
    "    scrolling_steps_count=20    \n",
    "    ):\n",
    "    web_driver = web_driver\n",
    "    url = url\n",
    "    web_driver.get(url)\n",
    "    sleep(sleep_interval)\n",
    "    print(\"scrolling to laod complete main table\")\n",
    "    scroll_down_slowly(web_driver,speed,scroll_sleep_interval,scrolling_steps_count)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dynamic_main_table_old(\n",
    "    web_driver,\n",
    "    url,\n",
    "    range_input=1000,\n",
    "    sleep_interval=5,\n",
    "    speed=300,\n",
    "    scroll_sleep_interval=3\n",
    "    ):\n",
    "    web_driver = web_driver\n",
    "    url = url\n",
    "    web_driver.get(url)\n",
    "    sleep(sleep_interval)\n",
    "    print(\"scrolling to laod complete main table\")\n",
    "    previous_length = 0\n",
    "    counter =0\n",
    "    for x in range (range_input):\n",
    "        print(\"scrolling down to end page\")\n",
    "        # web_driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # scroll_down_page(web_driver,speed,scroll_sleep_interval)\n",
    "        scroll_down_slowly(web_driver,speed,scroll_sleep_interval)\n",
    "        print(\"sleeping\")\n",
    "        sleep(1)\n",
    "        print(\"woke\")\n",
    "        results=web_driver.find_elements(by=By.XPATH, value='.//table[@class=\"table box table-striped\"]')\n",
    "        if previous_length <= len(results):\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "        previous_length = len(results)\n",
    "        table_html = results[-1].get_attribute(\"outerHTML\")\n",
    "        # check if table is empty\n",
    "        df = pd.read_html(table_html)[0]\n",
    "        if df.shape[0] == 0:\n",
    "            break\n",
    "        if counter > 20:\n",
    "            print(\"table loading finished\")\n",
    "            break\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_main_table_from_loaded_page(driver):\n",
    "    final_dfs=[]\n",
    "    print(\"retrieving all data.... \\n\")\n",
    "\n",
    "    #     get all table's data and concatenate them\n",
    "    results=driver.find_elements(by=By.XPATH, value='.//table[@class=\"table box table-striped\"]')\n",
    "    for each_table in results:\n",
    "        # Use Pandas to read HTML and convert it to a DataFrame\n",
    "        df = pd.read_html(each_table.get_attribute(\"outerHTML\"))[0]\n",
    "        links = []\n",
    "        for row_index, row in df.iterrows():\n",
    "            link_elements = each_table.find_elements(by=By.XPATH, value=f'.//tr[{row_index + 1}]/td[1]//a')\n",
    "            # Extract the href attribute from the first link element, if any\n",
    "            link = link_elements[0].get_attribute(\"href\") if link_elements else None\n",
    "            links.append(link)\n",
    "        # Add a new column to the DataFrame with the extracted links\n",
    "        df[\"Link\"] = links\n",
    "        final_dfs.append(df)\n",
    "    result_df=pd.concat(final_dfs, axis=0, ignore_index=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=\"../data/extra/clubranking/insidelacrosse\"\n",
    "if (not os.path.exists(data_folder)):\n",
    "    os.makedirs(data_folder)\n",
    "    print(f'new data folder {data_folder} created success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_ranking_main_url = \"https://www.insidelacrosse.com/recruiting/club\"\n",
    "gender_list=[\"girls\",\"boys\"]\n",
    "class_list = [x for x in range(2024,2029)]\n",
    "session_list = [x for x in range(2019,2025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create config file to track status and resume scraping\n",
    "if CREATE_CONFIG_FLAG:\n",
    "\n",
    "    urls = []\n",
    "    status=[]\n",
    "    for gender in gender_list:\n",
    "        for clas in class_list:\n",
    "            for session in session_list:\n",
    "                url_to_scrape = club_ranking_main_url+f'/{clas}/{session}/{gender}'\n",
    "                urls.append(url_to_scrape)\n",
    "                status.append('No')\n",
    "    scraping_status_df=pd.DataFrame({'url':urls,'status':status})\n",
    "    scraping_status_df.to_csv(os.path.join(data_folder,'input','url_scraping_status.csv'),index=False)\n",
    "    scraping_status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scraping status csv file\n",
    "scraping_status_df = pd.read_csv(os.path.join(data_folder,'input','url_scraping_status.csv'))\n",
    "# scraping_status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate webdriver\n",
    "driver = webdriver.Chrome(options=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over dataframe\n",
    "for index, row in scraping_status_df.iterrows():\n",
    "    if row['status'] in STATUS_VALUE_TO_SCRAPE:\n",
    "        # start scraping data\n",
    "        try:\n",
    "            url_to_scrape = row['url']\n",
    "            print(url_to_scrape)\n",
    "            load_dynamic_main_table(driver,row['url'],sleep_interval=5,speed=2000,scroll_sleep_interval=3)\n",
    "            output_df=fetch_main_table_from_loaded_page(driver)\n",
    "            file_to_save = url_to_scrape.split('club/')[-1].replace('/','_')+'.csv'\n",
    "            print(file_to_save)\n",
    "            if SAVE_DATA_FLAG:\n",
    "                output_df.to_csv(os.path.join(data_folder,'output/raw',file_to_save),index=False)\n",
    "            print('saved success')\n",
    "            scraping_status_df.loc[index,'status']='Yes'\n",
    "            del(output_df)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('data save failed')\n",
    "            scraping_status_df.loc[index,'status']='Fail'\n",
    "            # update status col with Failed\n",
    "            # scraping_status_df\n",
    "        if index > 2 and index%2 ==0:\n",
    "            if UPDATE_CONFIG_FLAG:\n",
    "                scraping_status_df.to_csv(os.path.join(data_folder,'input','url_scraping_status.csv'),index=False)\n",
    "            print(f'config file updated')\n",
    "    else:\n",
    "        # url =  row['url']\n",
    "        # status = row['status']\n",
    "        print(f\"skipping url: {row['url']} with status: {row['status']}\")\n",
    "    # if index >= 5:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dee183c1ab44ac1d4c1e637787782bfafcb5ea2442cd0a4291b50d441210934d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
