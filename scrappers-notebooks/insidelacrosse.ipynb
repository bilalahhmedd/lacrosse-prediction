{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "opt = webdriver.ChromeOptions()\n",
    "opt.add_argument(\"--start-maximized\")\n",
    "opt.add_experimental_option(\"excludeSwitches\", [\"disable-popup-blocking\"])\n",
    "# opt.add_argument(\"--headless\")\n",
    "# chromedriver_autoinstaller.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install chromedriver\n",
    "# !pip install chromedriver_autoinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scrape_links = {\n",
    "    \"INSIDE LACROSSE (GIRLS)\": \"https://www.insidelacrosse.com/recruiting/allcommitments/girls\",               \n",
    "    \"INSIDE LACROSSE (BOYS)\": \"https://www.insidelacrosse.com/recruiting/allcommitments\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_name=\"../data/insidelacrosse\"\n",
    "if (not os.path.exists(output_folder_name)):\n",
    "    os.mkdir(output_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dynamic_main_table(\n",
    "    web_driver,\n",
    "    url,\n",
    "    range_input=1000,\n",
    "    sleep_interval=1\n",
    "    ):\n",
    "    web_driver = web_driver\n",
    "    url = url\n",
    "    web_driver.get(url)\n",
    "    sleep(5)\n",
    "    print(\"scrolling to laod complete main table\")\n",
    "    previous_length = 0\n",
    "    counter =0\n",
    "    for x in range (range_input):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(sleep_interval)\n",
    "        results=driver.find_elements(by=By.XPATH, value='.//table[@class=\"table box table-striped\"]')\n",
    "        if previous_length <= len(results):\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "        previous_length = len(results)\n",
    "        table_html = results[-1].get_attribute(\"outerHTML\")\n",
    "        # check if table is empty\n",
    "        df = pd.read_html(table_html)[0]\n",
    "        if df.shape[0] == 0:\n",
    "            break\n",
    "        if counter > 20:\n",
    "            print(\"table loading finished\")\n",
    "            break\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_main_table_from_loaded_page():\n",
    "    final_dfs=[]\n",
    "    print(\"retrieving all data.... \\n\")\n",
    "\n",
    "    #     get all table's data and concatenate them\n",
    "    results=driver.find_elements(by=By.XPATH, value='.//table[@class=\"table box table-striped\"]')\n",
    "    for each_table in results:\n",
    "        # Use Pandas to read HTML and convert it to a DataFrame\n",
    "        df = pd.read_html(each_table.get_attribute(\"outerHTML\"))[0]\n",
    "        links = []\n",
    "        for row_index, row in df.iterrows():\n",
    "            link_elements = each_table.find_elements(by=By.XPATH, value=f'.//tr[{row_index + 1}]/td[1]//a')\n",
    "            # Extract the href attribute from the first link element, if any\n",
    "            link = link_elements[0].get_attribute(\"href\") if link_elements else None\n",
    "            links.append(link)\n",
    "        # Add a new column to the DataFrame with the extracted links\n",
    "        df[\"Link\"] = links\n",
    "        final_dfs.append(df)\n",
    "    result_df=pd.concat(final_dfs, axis=0, ignore_index=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_player_url_data(url):\n",
    "    driver = webdriver.Chrome(options=opt)  \n",
    "    driver.get(url+\"?active=teams\")\n",
    "    result_dict={}\n",
    "    try:\n",
    "        # Wait for the presence of the div \n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, './/table[@class=\"table table-striped box m-b-0\"]'))\n",
    "        )\n",
    "        sleep(1)\n",
    "        results=driver.find_elements(by=By.XPATH, value='.//table[@class=\"table table-striped box m-b-0\"]')\n",
    "        for table in results:\n",
    "            header_elements = table.find_elements(by=By.XPATH, value=\".//thead//th\")  # Adjust the XPath as needed\n",
    "\n",
    "            # Extract the text content from each header element\n",
    "            column_names = [header.text for header in header_elements]\n",
    "            if \"Club\" in column_names:\n",
    "                df = pd.read_html(table.get_attribute(\"outerHTML\"))[0]\n",
    "                result_dict = df.iloc[:, :2].to_dict(orient='list')\n",
    "                result_dict={f\"Season_{season}\": club for season, club in zip(result_dict['Season'], result_dict['Club'])}\n",
    "    except:\n",
    "        pass\n",
    "    driver.quit()\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_dynamic_main_table(driver,scrape_links['INSIDE LACROSSE (GIRLS)'],sleep_interval=3):\n",
    "    scrapped_df=fetch_main_table_from_loaded_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Season_2024': 'Yellow Jackets 2025 Lomo',\n",
       " 'Season_2023': 'Yellow Jackets 2025 Lomo',\n",
       " 'Season_2022': 'Yellow Jackets 2023 Rella'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_player_url_data(scrapped_df['Link'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load complete table page "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dee183c1ab44ac1d4c1e637787782bfafcb5ea2442cd0a4291b50d441210934d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
