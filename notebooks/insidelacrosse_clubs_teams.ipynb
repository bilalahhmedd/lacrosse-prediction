{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "import os\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# import chromedriver_autoinstaller\n",
    "# chromedriver_autoinstaller.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = webdriver.ChromeOptions()\n",
    "opt.add_argument(\"--start-maximized\")\n",
    "opt.add_experimental_option(\"excludeSwitches\", [\"disable-popup-blocking\"])\n",
    "# opt.add_argument(\"--headless\")\n",
    "# opt.add_argument(\"--lang=en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_each_team_players(team_class_list):\n",
    "    \"\"\" scrape each team players \"\"\"\n",
    "    driver = webdriver.Chrome(options=opt)\n",
    "    driver.get(team_class_list[-1])\n",
    "    df_player=pd.DataFrame()\n",
    "    try:\n",
    "        wait_for__player_table = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, './/table[@class=\"table box table-striped\"]'))\n",
    "        )\n",
    "        sleep(1)\n",
    "        \n",
    "        player_table=driver.find_element(by=By.XPATH, value='.//table[@class=\"table box table-striped\"]')\n",
    "        player_table\n",
    "        table_html = player_table.get_attribute('outerHTML')\n",
    "\n",
    "        # Convert HTML table to DataFrame\n",
    "        df_player = pd.read_html(table_html)[0]\n",
    "        rows = player_table.find_elements(By.XPATH, \".//tbody/tr\")\n",
    "\n",
    "        # Initialize an empty list to store the counts of full stars\n",
    "        full_star_counts = []\n",
    "\n",
    "        # Loop through each row\n",
    "        for row in rows:\n",
    "            # Find the last column in each row (rating column)\n",
    "            rating_column = row.find_element(By.XPATH, \".//td[last()]\")\n",
    "            # Count the number of full stars (icon-star-full elements)\n",
    "            full_star_count = rating_column.find_elements(By.XPATH, \".//i[@class='icon-star-full']\")\n",
    "            # Append the count to the list\n",
    "            full_star_counts.append(len(full_star_count))\n",
    "        df_player['Rating'] = full_star_counts\n",
    "        \n",
    "        \n",
    "        df_player[\"team_name\"]=team_class_list[0]\n",
    "        df_player[\"year\"]=team_class_list[1]\n",
    "        \n",
    "        \n",
    "    except:\n",
    "#         no table found\n",
    "        pass\n",
    "    driver.close()\n",
    "\n",
    "    return df_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_club_url(\n",
    "        club_url:str,\n",
    "        driver:object,\n",
    "        gender='boys'\n",
    "        ):\n",
    "    base_link=club_url\n",
    "    driver=driver\n",
    "    driver.get(base_link)\n",
    "    print(f\"scrpaing url: {club_url}\")\n",
    "    try:\n",
    "        wait_for_tables = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, './/table[@class=\"table table-striped box\"]'))\n",
    "        )\n",
    "        sleep(1)\n",
    "        results=driver.find_elements(by=By.XPATH, value='.//table[@class=\"table table-striped box\"]')\n",
    "        for each_table in results:\n",
    "            header = each_table.find_element(By.XPATH, './/tr[1]/th[1]').text\n",
    "            # find specific gender table \n",
    "            if gender.lower()  in header.lower():\n",
    "                #  get all team names, its links and class\n",
    "                team_names_cells=each_table.find_elements(By.XPATH, './/tr/td[1]')\n",
    "                team_names = [team.text for team in team_names_cells]\n",
    "                team_class_cells=each_table.find_elements(By.XPATH, './/tr/td[2]')\n",
    "                team_classes = [class_.text for class_ in team_class_cells]\n",
    "                hrefs = each_table.find_elements(By.XPATH, './/tr/td[1]//a[@href]')\n",
    "                # Extract href attributes and store them in a list of teams \n",
    "                href_values = [href.get_attribute('href') for href in hrefs]\n",
    "                merged_list = list(zip(team_names, team_classes, href_values))\n",
    "                list_of_lists_teams_n_class = [list(t) for t in merged_list]\n",
    "                \n",
    "                # scrape for each team in parallel\n",
    "\n",
    "                print(f\"scrapping for each team in parallel for   {str(len(list_of_lists_teams_n_class))} teams\")\n",
    "                with ThreadPoolExecutor(max_workers=5) as executor:  # Adjust max_workers as needed\n",
    "                    # Use executor.map to apply the function to each URL in parallel\n",
    "                    result_dfs = executor.map(scrape_each_team_players, list_of_lists_teams_n_class)\n",
    "                concatenated_df = pd.concat(result_dfs, ignore_index=True)\n",
    "                \n",
    "                print('success ... ')\n",
    "                return concatenated_df\n",
    "    except TimeoutException as error:\n",
    "        print('Caught Timout ' + repr(error))\n",
    "        # scrape_single_club_url(club_url=club_url,driver=driver,gender=gender)\n",
    "\n",
    "    except Exception as error:\n",
    "        print('Caught this error: ' + repr(error))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __main__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_gender=\"boys\"\n",
    "data_folder_path = \"../data/extra/insidelacrosse_clubs_teams/\"\n",
    "input_file_name=\"extracted_clubs_years_2017_2024.xlsx\"\n",
    "input_file_path =os.path.join(os.path.join(data_folder_path,'input',input_file_name))\n",
    "output_folder_name=f\"clubs_teams_{team_gender}\"\n",
    "output_folder_dir = os.path.join(data_folder_path,'output',output_folder_name)\n",
    "if (not os.path.exists(output_folder_dir)):\n",
    "    os.mkdir(output_folder_dir)\n",
    "    print('folder created: '+str(output_folder_dir))\n",
    "input_df=pd.read_excel(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_inputfile_flag=True\n",
    "starting_index=1000\n",
    "driver = webdriver.Chrome(options=opt)\n",
    "for index,row in input_df.iterrows():\n",
    "    if row['Status'] not in ['Yes','No'] and index > starting_index:\n",
    "        print(\"at index: \"+str(index))\n",
    "        result =scrape_single_club_url(club_url=row['Link'],driver=driver,gender='boys')\n",
    "        if result is None:\n",
    "            print(\"set status to No\")\n",
    "            input_df.iat[index,3]='No'\n",
    "        else:\n",
    "            result.to_csv(f'{output_folder_dir}/{row[\"Club Name\"].replace(\" \",\"_\")}_{str(row[\"Year\"])}.csv')\n",
    "            print(\"set status to Yes\")\n",
    "            input_df.iat[index,3]='Yes'\n",
    "        if update_inputfile_flag:\n",
    "            if index %10 ==0:   # update excel files after every 10 urls processed\n",
    "                input_df.to_excel(input_file_path,index=False)\n",
    "                print(\"input sheet updated at index: \"+str(index))\n",
    "\n",
    "print(\"completed....\")\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
